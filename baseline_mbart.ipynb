{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rsse_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "91Jjmu4ZUHF3",
        "kAJb690XUN9f",
        "sh_L7ID3UV5z",
        "L1ExEGtil1M2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzfDbSyqlviN"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpwkc9T7nf8c",
        "outputId": "8aa8b442-08d7-47d1-c643-b2022f16878e"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91Jjmu4ZUHF3"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbFv7t7qRX3R"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1dB3X-Wx8qU_5nDG_pxAmLvo5H_sgnHrE' \\\r\n",
        " -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1dB3X-Wx8qU_5nDG_pxAmLvo5H_sgnHrE\" -O train_wiki.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XstWUEnRVof"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=11lqipq6ggrgCk8bVxQ4-uuPVMCKN5ebU' \\\r\n",
        " -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=11lqipq6ggrgCk8bVxQ4-uuPVMCKN5ebU\" -O test_wiki.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CISb98JHQwlb"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1bJo8TagTGKa0uyppQRqsHrKHyYO5tcZc' \\\r\n",
        " -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1bJo8TagTGKa0uyppQRqsHrKHyYO5tcZc\" -O dev_wiki.csv && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAJb690XUN9f"
      },
      "source": [
        "## For huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmkLPDb3nozY"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers \r\n",
        "!pip install -e \".[dev]\"\r\n",
        "!pip install -r /content/transformers/examples/seq2seq/requirements.txt\r\n",
        "!pip install pyarrow==0.17.1\r\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh_L7ID3UV5z"
      },
      "source": [
        "## For Fairseq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ZKUnJeBtdN"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz\r\n",
        "!tar -xzvf /content/mbart.cc25.v2.tar.gz"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPmEjJOpB0g6"
      },
      "source": [
        "!apt-get install cmake build-essential pkg-config libgoogle-perftools-dev"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE7KoIEQ8VA2"
      },
      "source": [
        "!git clone https://github.com/google/sentencepiece.git "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9WzLBmn8buP"
      },
      "source": [
        "!(cd sentencepiece && mkdir build)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDo7EGTv8ero"
      },
      "source": [
        "!(cd sentencepiece/build && cmake .. && make && make install && ldconfig -v)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-vCBknt8kab"
      },
      "source": [
        "!git clone https://github.com/pytorch/fairseq\r\n",
        "!(cd fairseq && pip install --editable ./)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1ExEGtil1M2"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whqn5Jjal9Yv"
      },
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaIvkDjSzMqm"
      },
      "source": [
        "def df_to_pairs(path, split='train', lib='huggingface'):\r\n",
        "  tmp_df = pd.read_csv(path)\r\n",
        "  if lib == 'fairseq':\r\n",
        "    tmp_df_ru = tmp_df[['target_x', 'target_y']]\r\n",
        "    tmp_df_en = tmp_df[['src', 'dst']]\r\n",
        "    tmp_df_en.to_csv(\"/content/data/\" + split + '.en.txt', sep=str('\\t'), index=False, header=False)\r\n",
        "    tmp_df_ru.to_csv(\"/content/data/\" + split + '.ru.txt', sep=str('\\t'), index=False, header=False)\r\n",
        "  elif lib == 'huggingface':\r\n",
        "    tmp_df_source = tmp_df['target_x']\r\n",
        "    tmp_df_source = tmp_df_source.append(tmp_df['src'], ignore_index=True)\r\n",
        "    tmp_df_source = tmp_df_source.reindex(np.random.permutation(tmp_df_source.index))\r\n",
        "    tmp_df_source.to_csv(\"/content/data/\" + split + \".source\")\r\n",
        "    tmp_df_target = tmp_df['target_y']\r\n",
        "    tmp_df_target = tmp_df_target.append(tmp_df['dst'], ignore_index=True)\r\n",
        "    tmp_df_target = tmp_df_target.reindex(np.random.permutation(tmp_df_target.index))\r\n",
        "    tmp_df_target.to_csv(\"/content/data/\" + split + \".target\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B38f7lDWnTGy"
      },
      "source": [
        "df_to_pairs('/content/train_wiki.csv')\r\n",
        "df_to_pairs('/content/dev_wiki.csv', split='valid')\r\n",
        "df_to_pairs('/content/test_wiki.csv', split='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvTQWYU_l6T9"
      },
      "source": [
        "# FairSeq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHujCgshFCu5"
      },
      "source": [
        "# !SPM=\"/content/sentencepiece/build/src/spm_encode\"\r\n",
        "# !BPE_MODEL=\"/content/mbart.cc25.v2/sentence.bpe.model\"\r\n",
        "# !DATA_DIR=\"/content/data\"\r\n",
        "# !SRC=en\r\n",
        "# !TGT=ru\r\n",
        "!/content/sentencepiece/build/src/spm_encode --model=/content/mbart.cc25.v2/sentence.bpe.model < /content/data/train.en > /content/data/train.spm.en &\r\n",
        "!/content/sentencepiece/build/src/spm_encode --model=/content/mbart.cc25.v2/sentence.bpe.model  < /content/data/train.ru > /content/data/train.spm.ru \r\n",
        "!/content/sentencepiece/build/src/spm_encode --model=/content/mbart.cc25.v2/sentence.bpe.model  < /content/data/valid.en > /content/data/valid.spm.en &\r\n",
        "!/content/sentencepiece/build/src/spm_encode --model=/content/mbart.cc25.v2/sentence.bpe.model  < /content/data/valid.ru > /content/data/valid.spm.ru &\r\n",
        "!/content/sentencepiece/build/src/spm_encode --model=/content/mbart.cc25.v2/sentence.bpe.model  < /content/data/test.en > /content/data/test.spm.en &\r\n",
        "!/content/sentencepiece/build/src/spm_encode --model=/content/mbart.cc25.v2/sentence.bpe.model < /content/data/test.ru > /content/data/test.spm.ru &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyyizgMtAmRg"
      },
      "source": [
        "# PREPROCESSED_DATA_DIR=/directory/to/save/preprocessed/data\r\n",
        "# DICT=/path/to/downloaded/mbart/model/directory/dict.txt\r\n",
        "!fairseq-preprocess \\\r\n",
        "  --source-lang en \\\r\n",
        "  --target-lang ru \\\r\n",
        "  --trainpref /content/data/train.spm \\\r\n",
        "  --validpref /content/data/valid.spm \\\r\n",
        "  --testpref /content/data/test.spm \\\r\n",
        "  --destdir /content/data \\\r\n",
        "  --thresholdtgt 0 \\\r\n",
        "  --thresholdsrc 0 \\\r\n",
        "  --srcdict /content/mbart.cc25.v2/dict.txt \\\r\n",
        "  --tgtdict /content/mbart.cc25.v2/dict.txt \\\r\n",
        "  --workers 70"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeAZMXICI69p"
      },
      "source": [
        "# !(cd /content/fairseq  &&  git checkout `git rev-list -1 --before=\"Dec 30 2020\" master`)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfdREINBKCJa"
      },
      "source": [
        "# !(cd /content/fairseq/ && pip install --editable . && python setup.py build develop)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCaaB1z7A8l9",
        "outputId": "107be0a6-f36a-4b10-f36d-cfd15af89be1"
      },
      "source": [
        "# PRETRAIN=/path/to/downloaded/mbart/model/directory/model.pt\r\n",
        "!langs=ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN\r\n",
        "!CUDA_VISIBLE_DEVICES=0,1,2,3\r\n",
        "# !SAVE_DIR=/path/to/save/model/checkpoint\r\n",
        "!fairseq-train /content/data \\\r\n",
        "  --encoder-normalize-before --decoder-normalize-before \\\r\n",
        "  --arch mbart_large --layernorm-embedding \\\r\n",
        "  --task translation_from_pretrained_bart \\\r\n",
        "  --criterion label_smoothed_cross_entropy --label-smoothing 0.2 \\\r\n",
        "  --optimizer adam --adam-eps 1e-06 --adam-betas '(0.9, 0.98)' \\\r\n",
        "  --lr-scheduler polynomial_decay --lr 3e-05 --warmup-updates 2500 --total-num-update 54725  \\\r\n",
        "  --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \\\r\n",
        "  --max-tokens 1024 --update-freq 2 \\\r\n",
        "  --source-lang en --target-lang ru \\\r\n",
        "  --batch-size 16 \\\r\n",
        "  --validate-interval 1 \\\r\n",
        "  --patience 3 \\\r\n",
        "  --max-epoch 25 \\\r\n",
        "  --save-interval 5 --keep-last-epochs 10 --keep-best-checkpoints 2 \\\r\n",
        "  --seed 42 --log-format simple --log-interval 500 \\\r\n",
        "  --restore-file /content/mbart.cc25.v2/model.pt \\\r\n",
        "  --reset-optimizer --reset-meters --reset-dataloader --reset-lr-scheduler \\\r\n",
        "  --ddp-backend no_c10d \\\r\n",
        "  --langs ar_AR,cs_CZ,de_DE,en_XX,es_XX,et_EE,fi_FI,fr_XX,gu_IN,hi_IN,it_IT,ja_XX,kk_KZ,ko_KR,lt_LT,lv_LV,my_MM,ne_NP,nl_XX,ro_RO,ru_RU,si_LK,tr_TR,vi_VN,zh_CN \\\r\n",
        "  --scoring bleu \\\r\n",
        "  --save-dir /content/checkpoints > train_log.txt &"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/usr/local/bin/fairseq-train\", line 25, in importlib_load_entry_point\n",
            "    return next(matches).load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/importlib_metadata/__init__.py\", line 96, in load\n",
            "    module = import_module(match.group('module'))\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 19, in <module>\n",
            "    from fairseq import (\n",
            "  File \"/content/fairseq/fairseq/__init__.py\", line 32, in <module>\n",
            "    import fairseq.criterions  # noqa\n",
            "  File \"/content/fairseq/fairseq/criterions/__init__.py\", line 36, in <module>\n",
            "    importlib.import_module(\"fairseq.criterions.\" + file_name)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/fairseq/fairseq/criterions/ctc.py\", line 19, in <module>\n",
            "    from fairseq.tasks import FairseqTask\n",
            "  File \"/content/fairseq/fairseq/tasks/__init__.py\", line 116, in <module>\n",
            "    module = importlib.import_module(\"fairseq.tasks.\" + task_name)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/fairseq/fairseq/tasks/multilingual_translation.py\", line 19, in <module>\n",
            "    from fairseq.models import FairseqMultiModel\n",
            "  File \"/content/fairseq/fairseq/models/__init__.py\", line 208, in <module>\n",
            "    module = importlib.import_module(\"fairseq.models.\" + model_name)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/fairseq/fairseq/models/speech_to_text/__init__.py\", line 7, in <module>\n",
            "    from .convtransformer import *  # noqa\n",
            "  File \"/content/fairseq/fairseq/models/speech_to_text/convtransformer.py\", line 10, in <module>\n",
            "    from examples.simultaneous_translation.utils.data_utils import (\n",
            "ModuleNotFoundError: No module named 'examples.simultaneous_translation'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zyyC0bmpSFn"
      },
      "source": [
        "# HuggingFace\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43FYFsSvqx8p"
      },
      "source": [
        "from tqdm.auto import tqdm\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import DistributedSampler, RandomSampler\r\n",
        "\r\n",
        "import logging\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import copy\r\n",
        "from dataclasses import dataclass, field\r\n",
        "from typing import Optional\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from transformers import (\r\n",
        "    AutoModelForSeq2SeqLM,\r\n",
        "    AutoTokenizer,\r\n",
        "    HfArgumentParser,\r\n",
        "    MBartTokenizer,\r\n",
        "    TrainingArguments,\r\n",
        "    set_seed,\r\n",
        "    Trainer,\r\n",
        ")\r\n",
        "\r\n",
        "from transformers.trainer_utils import EvaluationStrategy\r\n",
        "from transformers.optimization import (\r\n",
        "    Adafactor,\r\n",
        "    AdamW,\r\n",
        "    get_constant_schedule,\r\n",
        "    get_constant_schedule_with_warmup,\r\n",
        "    get_cosine_schedule_with_warmup,\r\n",
        "    get_cosine_with_hard_restarts_schedule_with_warmup,\r\n",
        "    get_linear_schedule_with_warmup,\r\n",
        "    get_polynomial_decay_schedule_with_warmup,\r\n",
        ")\r\n",
        "\r\n",
        "from typing import Any, Dict, Optional, Tuple, Union\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "from torch.utils.data import DistributedSampler, RandomSampler\r\n",
        "\r\n",
        "from transformers import PreTrainedModel, Trainer, logging\r\n",
        "from transformers.file_utils import is_torch_tpu_available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKMe8qwSVMe9"
      },
      "source": [
        "(I used previous seq2seq examples, because it is what I worked with before.)\r\n",
        "\r\n",
        "((Also, to make it work without problems, I commented lines that were about git in \"transformers/examples/legacy/seq2seq/\"))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s17tx1SupUQi",
        "outputId": "e55e45bb-c3ac-4d32-9bed-09e078d8c5c6"
      },
      "source": [
        "!python '/content/transformers/examples/legacy/seq2seq/finetune_trainer.py' \\\r\n",
        "    --data_dir '/content/data' \\\r\n",
        "    --model_name_or_path 'google/mt5-small' \\\r\n",
        "    --tokenizer_name  'google/mt5-small' \\\r\n",
        "    --output_dir \"/content/drive/MyDrive/rsse_model\" \\\r\n",
        "    --num_train_epochs 1 --do_train \\\r\n",
        "    --n_val 5000 \\\r\n",
        "    --freeze_embeds --freeze_encoder \\\r\n",
        "    # --overwrite_output_dir \r\n",
        "        # --model_name_or_path 'google/mt5-small' \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-21 15:56:35.544452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/21/2021 15:56:38 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/21/2021 15:56:38 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='/content/drive/MyDrive/rsse_model', overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Feb21_15-56-38_745c92ba1a0d', logging_strategy=<LoggingStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='/content/drive/MyDrive/rsse_model', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, label_smoothing=0.0, sortish_sampler=False, predict_with_generate=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear')\n",
            "02/21/2021 15:56:38 - INFO - filelock -   Lock 140490226674376 acquired on /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4.lock\n",
            "[INFO|file_utils.py:1325] 2021-02-21 15:56:38,775 >> https://huggingface.co/google/mt5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbe9zctsa\n",
            "Downloading: 100% 553/553 [00:00<00:00, 554kB/s]\n",
            "[INFO|file_utils.py:1329] 2021-02-21 15:56:38,980 >> storing https://huggingface.co/google/mt5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "[INFO|file_utils.py:1332] 2021-02-21 15:56:38,980 >> creating metadata file for /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "02/21/2021 15:56:38 - INFO - filelock -   Lock 140490226674376 released on /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4.lock\n",
            "[INFO|configuration_utils.py:456] 2021-02-21 15:56:38,981 >> loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "[INFO|configuration_utils.py:492] 2021-02-21 15:56:38,982 >> Model config MT5Config {\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:456] 2021-02-21 15:56:39,186 >> loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "[INFO|configuration_utils.py:492] 2021-02-21 15:56:39,187 >> Model config MT5Config {\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1688] 2021-02-21 15:56:39,187 >> Model name 'google/mt5-small' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming 'google/mt5-small' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/21/2021 15:56:39 - INFO - filelock -   Lock 140490660555520 acquired on /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2.lock\n",
            "[INFO|file_utils.py:1325] 2021-02-21 15:56:39,432 >> https://huggingface.co/google/mt5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpysvragkq\n",
            "Downloading: 100% 4.31M/4.31M [00:00<00:00, 8.42MB/s]\n",
            "[INFO|file_utils.py:1329] 2021-02-21 15:56:40,191 >> storing https://huggingface.co/google/mt5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
            "[INFO|file_utils.py:1332] 2021-02-21 15:56:40,191 >> creating metadata file for /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
            "02/21/2021 15:56:40 - INFO - filelock -   Lock 140490660555520 released on /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2.lock\n",
            "02/21/2021 15:56:40 - INFO - filelock -   Lock 140490660555520 acquired on /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276.lock\n",
            "[INFO|file_utils.py:1325] 2021-02-21 15:56:40,806 >> https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpb4u6rvh_\n",
            "Downloading: 100% 99.0/99.0 [00:00<00:00, 90.1kB/s]\n",
            "[INFO|file_utils.py:1329] 2021-02-21 15:56:41,010 >> storing https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
            "[INFO|file_utils.py:1332] 2021-02-21 15:56:41,010 >> creating metadata file for /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
            "02/21/2021 15:56:41 - INFO - filelock -   Lock 140490660555520 released on /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276.lock\n",
            "02/21/2021 15:56:41 - INFO - filelock -   Lock 140490660555520 acquired on /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32.lock\n",
            "[INFO|file_utils.py:1325] 2021-02-21 15:56:41,215 >> https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp95ag8wdx\n",
            "Downloading: 100% 82.0/82.0 [00:00<00:00, 78.3kB/s]\n",
            "[INFO|file_utils.py:1329] 2021-02-21 15:56:41,420 >> storing https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
            "[INFO|file_utils.py:1332] 2021-02-21 15:56:41,420 >> creating metadata file for /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
            "02/21/2021 15:56:41 - INFO - filelock -   Lock 140490660555520 released on /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32.lock\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-21 15:56:41,421 >> loading file https://huggingface.co/google/mt5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-21 15:56:41,421 >> loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-21 15:56:41,421 >> loading file https://huggingface.co/google/mt5-small/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-21 15:56:41,421 >> loading file https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
            "[INFO|tokenization_utils_base.py:1786] 2021-02-21 15:56:41,421 >> loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
            "02/21/2021 15:56:42 - INFO - filelock -   Lock 140490660555520 acquired on /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70.lock\n",
            "[INFO|file_utils.py:1325] 2021-02-21 15:56:42,675 >> https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6_2uh1z2\n",
            "Downloading: 100% 1.20G/1.20G [00:13<00:00, 86.2MB/s]\n",
            "[INFO|file_utils.py:1329] 2021-02-21 15:56:56,647 >> storing https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
            "[INFO|file_utils.py:1332] 2021-02-21 15:56:56,647 >> creating metadata file for /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
            "02/21/2021 15:56:56 - INFO - filelock -   Lock 140490660555520 released on /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70.lock\n",
            "[INFO|modeling_utils.py:1035] 2021-02-21 15:56:56,647 >> loading weights file https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
            "[INFO|modeling_utils.py:1151] 2021-02-21 15:57:03,081 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1160] 2021-02-21 15:57:03,081 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
            "02/21/2021 15:57:18 - INFO - __main__ -   *** Train ***\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:788: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:889] 2021-02-21 15:57:18,445 >> ***** Running training *****\n",
            "[INFO|trainer.py:890] 2021-02-21 15:57:18,445 >>   Num examples = 493956\n",
            "[INFO|trainer.py:891] 2021-02-21 15:57:18,445 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:892] 2021-02-21 15:57:18,445 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:893] 2021-02-21 15:57:18,445 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:894] 2021-02-21 15:57:18,445 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:895] 2021-02-21 15:57:18,445 >>   Total optimization steps = 61745\n",
            "{'loss': 10.5541, 'learning_rate': 4.9595108915701675e-05, 'epoch': 0.01}\n",
            "  1% 500/61745 [02:07<4:10:50,  4.07it/s][INFO|trainer.py:1503] 2021-02-21 15:59:25,500 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 15:59:25,510 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 15:59:32,481 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 15:59:32,487 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 15:59:32,490 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 15:59:33,327 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-500/spiece.model\n",
            "{'loss': 5.6663, 'learning_rate': 4.9190217831403355e-05, 'epoch': 0.02}\n",
            "  2% 1000/61745 [04:58<4:39:21,  3.62it/s][INFO|trainer.py:1503] 2021-02-21 16:02:16,560 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-1000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:02:16,567 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:02:23,244 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:02:23,251 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:02:23,257 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:02:23,978 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-1000/spiece.model\n",
            "{'loss': 5.2893, 'learning_rate': 4.8785326747105035e-05, 'epoch': 0.02}\n",
            "  2% 1500/61745 [07:50<3:58:31,  4.21it/s][INFO|trainer.py:1503] 2021-02-21 16:05:09,318 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-1500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:05:09,329 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:05:15,723 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:05:15,898 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:05:15,905 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:05:16,577 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-1500/spiece.model\n",
            "{'loss': 5.1193, 'learning_rate': 4.838043566280671e-05, 'epoch': 0.03}\n",
            "  3% 2000/61745 [10:45<4:27:23,  3.72it/s][INFO|trainer.py:1503] 2021-02-21 16:08:03,513 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-2000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:08:03,523 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:08:09,965 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:08:09,972 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:08:09,990 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:08:10,673 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-2000/spiece.model\n",
            "{'loss': 4.9991, 'learning_rate': 4.797554457850839e-05, 'epoch': 0.04}\n",
            "  4% 2500/61745 [13:32<4:35:09,  3.59it/s][INFO|trainer.py:1503] 2021-02-21 16:10:51,444 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-2500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:10:51,451 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:10:57,537 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:10:57,544 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:10:57,548 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:10:58,264 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-2500/spiece.model\n",
            "{'loss': 4.9257, 'learning_rate': 4.757065349421006e-05, 'epoch': 0.05}\n",
            "  5% 3000/61745 [16:24<4:26:58,  3.67it/s][INFO|trainer.py:1503] 2021-02-21 16:13:43,154 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-3000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:13:43,163 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:13:49,836 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:13:49,842 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:13:49,846 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-3000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:13:50,546 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-3000/spiece.model\n",
            "{'loss': 4.8638, 'learning_rate': 4.716576240991173e-05, 'epoch': 0.06}\n",
            "  6% 3500/61745 [19:16<4:27:03,  3.64it/s][INFO|trainer.py:1503] 2021-02-21 16:16:34,622 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-3500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:16:34,631 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:16:41,076 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:16:41,082 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:16:41,086 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-3500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:16:41,755 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-3500/spiece.model\n",
            "{'loss': 4.7874, 'learning_rate': 4.676087132561341e-05, 'epoch': 0.06}\n",
            "  6% 4000/61745 [22:08<3:55:07,  4.09it/s][INFO|trainer.py:1503] 2021-02-21 16:19:26,989 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-4000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:19:27,001 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:19:33,905 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:19:33,926 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:19:33,931 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-4000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:19:34,750 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-4000/spiece.model\n",
            "{'loss': 4.7389, 'learning_rate': 4.635598024131509e-05, 'epoch': 0.07}\n",
            "  7% 4500/61745 [25:03<4:12:48,  3.77it/s][INFO|trainer.py:1503] 2021-02-21 16:22:22,170 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-4500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:22:22,179 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:22:28,876 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:22:29,266 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:22:29,271 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-4500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:22:29,942 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-4500/spiece.model\n",
            "{'loss': 4.6891, 'learning_rate': 4.5951089157016765e-05, 'epoch': 0.08}\n",
            "  8% 5000/61745 [27:58<4:32:04,  3.48it/s][INFO|trainer.py:1503] 2021-02-21 16:25:16,618 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-5000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:25:16,631 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:25:23,372 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:25:23,382 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:25:23,760 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:25:24,476 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-5000/spiece.model\n",
            "{'loss': 4.6304, 'learning_rate': 4.5546198072718445e-05, 'epoch': 0.09}\n",
            "  9% 5500/61745 [30:52<4:16:53,  3.65it/s][INFO|trainer.py:1503] 2021-02-21 16:28:10,940 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-5500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:28:10,948 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:28:17,719 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:28:17,731 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:28:17,735 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-5500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:28:18,542 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-5500/spiece.model\n",
            "{'loss': 4.6358, 'learning_rate': 4.514130698842012e-05, 'epoch': 0.1}\n",
            " 10% 6000/61745 [33:42<4:19:44,  3.58it/s][INFO|trainer.py:1503] 2021-02-21 16:31:00,848 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-6000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:31:00,856 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:31:07,107 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:31:07,123 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:31:07,129 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-6000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:31:07,933 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-6000/spiece.model\n",
            "{'loss': 4.585, 'learning_rate': 4.473641590412179e-05, 'epoch': 0.11}\n",
            " 11% 6500/61745 [36:34<3:50:03,  4.00it/s][INFO|trainer.py:1503] 2021-02-21 16:33:53,462 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-6500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:33:53,470 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:33:59,834 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:33:59,841 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:33:59,846 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-6500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:34:00,621 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-6500/spiece.model\n",
            "{'loss': 4.5558, 'learning_rate': 4.433152481982347e-05, 'epoch': 0.11}\n",
            " 11% 7000/61745 [39:30<4:04:54,  3.73it/s][INFO|trainer.py:1503] 2021-02-21 16:36:48,826 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-7000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:36:48,926 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:37:05,227 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:37:05,293 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:37:05,839 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-7000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:37:06,654 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-7000/spiece.model\n",
            "{'loss': 4.5408, 'learning_rate': 4.392663373552515e-05, 'epoch': 0.12}\n",
            " 12% 7500/61745 [42:27<4:30:34,  3.34it/s][INFO|trainer.py:1503] 2021-02-21 16:39:45,477 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-7500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:39:45,580 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:40:07,188 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:40:07,205 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:40:07,291 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-7500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:40:08,588 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-7500/spiece.model\n",
            "{'loss': 4.4949, 'learning_rate': 4.352174265122682e-05, 'epoch': 0.13}\n",
            " 13% 8000/61745 [45:25<3:56:53,  3.78it/s][INFO|trainer.py:1503] 2021-02-21 16:42:44,018 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-8000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:42:44,029 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:42:50,029 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:42:50,036 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:42:50,040 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-8000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:42:50,829 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-8000/spiece.model\n",
            "{'loss': 4.4858, 'learning_rate': 4.3116851566928496e-05, 'epoch': 0.14}\n",
            " 14% 8500/61745 [48:19<4:10:53,  3.54it/s][INFO|trainer.py:1503] 2021-02-21 16:45:37,848 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-8500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:45:37,954 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:46:13,845 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:46:14,308 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:46:14,313 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-8500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:46:15,085 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-8500/spiece.model\n",
            "{'loss': 4.4598, 'learning_rate': 4.2711960482630176e-05, 'epoch': 0.15}\n",
            " 15% 9000/61745 [51:26<4:00:10,  3.66it/s][INFO|trainer.py:1503] 2021-02-21 16:48:45,028 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-9000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:48:45,130 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:48:54,996 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:48:55,001 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:48:55,008 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-9000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:48:55,845 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-9000/spiece.model\n",
            "{'loss': 4.4537, 'learning_rate': 4.230706939833185e-05, 'epoch': 0.15}\n",
            " 15% 9500/61745 [54:23<3:57:44,  3.66it/s][INFO|trainer.py:1503] 2021-02-21 16:51:41,629 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-9500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:51:41,642 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:51:47,724 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:51:47,731 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:51:47,735 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-9500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:51:48,542 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-9500/spiece.model\n",
            "{'loss': 4.4456, 'learning_rate': 4.190217831403352e-05, 'epoch': 0.16}\n",
            " 16% 10000/61745 [57:11<4:04:29,  3.53it/s][INFO|trainer.py:1503] 2021-02-21 16:54:30,363 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-10000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:54:30,372 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:54:37,127 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:54:37,150 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:54:37,155 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-10000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:54:37,919 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-10000/spiece.model\n",
            "{'loss': 4.3844, 'learning_rate': 4.149728722973521e-05, 'epoch': 0.17}\n",
            " 17% 10500/61745 [1:00:02<4:01:29,  3.54it/s][INFO|trainer.py:1503] 2021-02-21 16:57:21,110 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-10500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 16:57:21,119 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 16:57:27,776 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 16:57:27,782 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 16:57:27,787 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-10500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 16:57:28,557 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-10500/spiece.model\n",
            "{'loss': 4.3734, 'learning_rate': 4.109239614543688e-05, 'epoch': 0.18}\n",
            " 18% 11000/61745 [1:02:51<3:37:21,  3.89it/s][INFO|trainer.py:1503] 2021-02-21 17:00:10,061 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-11000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:00:10,070 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:00:16,231 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:00:16,241 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:00:16,246 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-11000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:00:17,046 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-11000/spiece.model\n",
            "{'loss': 4.3602, 'learning_rate': 4.0687505061138554e-05, 'epoch': 0.19}\n",
            " 19% 11500/61745 [1:05:40<4:00:06,  3.49it/s][INFO|trainer.py:1503] 2021-02-21 17:02:59,090 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-11500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:02:59,098 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:03:05,739 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:03:05,745 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:03:05,750 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-11500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:03:06,526 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-11500/spiece.model\n",
            "{'loss': 4.3555, 'learning_rate': 4.028261397684023e-05, 'epoch': 0.19}\n",
            " 19% 12000/61745 [1:08:33<3:48:39,  3.63it/s][INFO|trainer.py:1503] 2021-02-21 17:05:51,969 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-12000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:05:51,986 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:05:58,749 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:05:58,759 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:05:58,765 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-12000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:05:59,541 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-12000/spiece.model\n",
            "{'loss': 4.3576, 'learning_rate': 3.9877722892541906e-05, 'epoch': 0.2}\n",
            " 20% 12500/61745 [1:11:23<3:47:47,  3.60it/s][INFO|trainer.py:1503] 2021-02-21 17:08:41,831 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-12500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:08:41,841 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:08:48,903 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:08:49,344 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:08:49,352 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-12500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:08:50,126 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-12500/spiece.model\n",
            "{'loss': 4.328, 'learning_rate': 3.947283180824358e-05, 'epoch': 0.21}\n",
            " 21% 13000/61745 [1:14:11<3:39:32,  3.70it/s][INFO|trainer.py:1503] 2021-02-21 17:11:30,148 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-13000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:11:30,157 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:11:36,874 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:11:36,915 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:11:36,921 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-13000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:11:37,685 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-13000/spiece.model\n",
            "{'loss': 4.346, 'learning_rate': 3.9067940723945265e-05, 'epoch': 0.22}\n",
            " 22% 13500/61745 [1:17:01<3:24:15,  3.94it/s][INFO|trainer.py:1503] 2021-02-21 17:14:20,455 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-13500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:14:20,467 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:14:27,061 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:14:27,067 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:14:27,072 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-13500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:14:27,861 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-13500/spiece.model\n",
            "{'loss': 4.3177, 'learning_rate': 3.866304963964694e-05, 'epoch': 0.23}\n",
            " 23% 14000/61745 [1:19:51<3:39:06,  3.63it/s][INFO|trainer.py:1503] 2021-02-21 17:17:09,594 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-14000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:17:09,603 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:17:16,101 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:17:16,111 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:17:16,116 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-14000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:17:16,877 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-14000/spiece.model\n",
            "{'loss': 4.3245, 'learning_rate': 3.825815855534861e-05, 'epoch': 0.23}\n",
            " 23% 14500/61745 [1:22:44<3:29:49,  3.75it/s][INFO|trainer.py:1503] 2021-02-21 17:20:03,115 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-14500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:20:03,130 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:20:09,059 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:20:09,069 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:20:09,075 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-14500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:20:09,877 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-14500/spiece.model\n",
            "{'loss': 4.2897, 'learning_rate': 3.785326747105029e-05, 'epoch': 0.24}\n",
            " 24% 15000/61745 [1:25:36<3:35:48,  3.61it/s][INFO|trainer.py:1503] 2021-02-21 17:22:55,122 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-15000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:22:55,131 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:23:01,711 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:23:01,717 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:23:01,734 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-15000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:23:02,493 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-15000/spiece.model\n",
            "{'loss': 4.2688, 'learning_rate': 3.7448376386751964e-05, 'epoch': 0.25}\n",
            " 25% 15500/61745 [1:28:28<4:05:46,  3.14it/s][INFO|trainer.py:1503] 2021-02-21 17:25:47,361 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-15500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:25:47,370 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:25:53,898 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:25:53,906 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:25:53,910 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-15500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:25:54,643 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-15500/spiece.model\n",
            "{'loss': 4.2895, 'learning_rate': 3.704348530245364e-05, 'epoch': 0.26}\n",
            " 26% 16000/61745 [1:31:16<3:22:48,  3.76it/s][INFO|trainer.py:1503] 2021-02-21 17:28:34,811 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-16000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:28:34,834 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:28:41,258 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:28:41,264 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:28:41,269 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-16000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:28:42,093 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-16000/spiece.model\n",
            "{'loss': 4.2735, 'learning_rate': 3.663859421815532e-05, 'epoch': 0.27}\n",
            " 27% 16500/61745 [1:34:09<3:21:18,  3.75it/s][INFO|trainer.py:1503] 2021-02-21 17:31:27,504 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-16500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:31:27,513 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:31:34,181 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:31:34,186 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:31:34,191 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-16500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:31:34,942 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-16500/spiece.model\n",
            "{'loss': 4.2582, 'learning_rate': 3.6233703133856996e-05, 'epoch': 0.28}\n",
            " 28% 17000/61745 [1:36:57<3:33:33,  3.49it/s][INFO|trainer.py:1503] 2021-02-21 17:34:16,393 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-17000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:34:16,409 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:34:22,971 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:34:22,978 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:34:22,983 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-17000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:34:23,815 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-17000/spiece.model\n",
            "{'loss': 4.2607, 'learning_rate': 3.582881204955867e-05, 'epoch': 0.28}\n",
            " 28% 17500/61745 [1:39:50<3:31:11,  3.49it/s][INFO|trainer.py:1503] 2021-02-21 17:37:08,583 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-17500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:37:08,591 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:37:15,607 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:37:15,643 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:37:15,648 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-17500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:37:16,414 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-17500/spiece.model\n",
            "{'loss': 4.2873, 'learning_rate': 3.542392096526035e-05, 'epoch': 0.29}\n",
            " 29% 18000/61745 [1:42:42<3:02:44,  3.99it/s][INFO|trainer.py:1503] 2021-02-21 17:40:00,895 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-18000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:40:00,915 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:40:07,596 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:40:07,602 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:40:07,607 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-18000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:40:08,437 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-18000/spiece.model\n",
            "{'loss': 4.2184, 'learning_rate': 3.501902988096202e-05, 'epoch': 0.3}\n",
            " 30% 18500/61745 [1:45:34<3:04:02,  3.92it/s][INFO|trainer.py:1503] 2021-02-21 17:42:52,711 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-18500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:42:52,730 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:43:00,032 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:43:00,036 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:43:00,040 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-18500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:43:00,821 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-18500/spiece.model\n",
            "{'loss': 4.2548, 'learning_rate': 3.4614138796663694e-05, 'epoch': 0.31}\n",
            " 31% 19000/61745 [1:48:25<3:22:23,  3.52it/s][INFO|trainer.py:1503] 2021-02-21 17:45:44,189 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-19000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:45:44,199 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:45:50,772 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:45:50,779 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:45:50,783 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-19000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:45:51,557 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-19000/spiece.model\n",
            "{'loss': 4.2129, 'learning_rate': 3.420924771236538e-05, 'epoch': 0.32}\n",
            " 32% 19500/61745 [1:51:19<2:55:02,  4.02it/s][INFO|trainer.py:1503] 2021-02-21 17:48:37,651 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-19500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:48:37,663 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:48:44,349 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:48:44,356 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:48:44,362 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-19500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:48:45,170 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-19500/spiece.model\n",
            "{'loss': 4.2286, 'learning_rate': 3.3804356628067054e-05, 'epoch': 0.32}\n",
            " 32% 20000/61745 [1:54:10<3:01:26,  3.83it/s][INFO|trainer.py:1503] 2021-02-21 17:51:29,457 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-20000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:51:29,466 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:51:35,474 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:51:35,479 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:51:35,488 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-20000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:51:36,319 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-20000/spiece.model\n",
            "{'loss': 4.2218, 'learning_rate': 3.339946554376873e-05, 'epoch': 0.33}\n",
            " 33% 20500/61745 [1:57:04<3:00:01,  3.82it/s][INFO|trainer.py:1503] 2021-02-21 17:54:22,589 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-20500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:54:22,598 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:54:29,184 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:54:29,189 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:54:29,194 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-20500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:54:29,998 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-20500/spiece.model\n",
            "{'loss': 4.2194, 'learning_rate': 3.2994574459470406e-05, 'epoch': 0.34}\n",
            " 34% 21000/61745 [1:59:57<2:54:05,  3.90it/s][INFO|trainer.py:1503] 2021-02-21 17:57:15,855 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-21000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 17:57:15,863 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 17:57:21,919 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 17:57:21,927 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 17:57:22,466 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-21000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 17:57:23,176 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-21000/spiece.model\n",
            "{'loss': 4.2193, 'learning_rate': 3.258968337517208e-05, 'epoch': 0.35}\n",
            " 35% 21500/61745 [2:02:48<3:18:06,  3.39it/s][INFO|trainer.py:1503] 2021-02-21 18:00:06,501 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-21500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:00:06,510 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:00:13,038 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:00:13,077 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:00:13,095 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-21500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:00:13,857 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-21500/spiece.model\n",
            "{'loss': 4.2112, 'learning_rate': 3.218479229087375e-05, 'epoch': 0.36}\n",
            " 36% 22000/61745 [2:05:38<2:52:08,  3.85it/s][INFO|trainer.py:1503] 2021-02-21 18:02:57,136 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-22000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:02:57,156 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:03:04,127 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:03:04,134 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:03:04,138 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-22000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:03:04,942 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-22000/spiece.model\n",
            "{'loss': 4.1985, 'learning_rate': 3.177990120657544e-05, 'epoch': 0.36}\n",
            " 36% 22500/61745 [2:08:32<3:12:24,  3.40it/s][INFO|trainer.py:1503] 2021-02-21 18:05:50,660 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-22500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:05:50,669 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:05:57,326 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:05:57,333 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:05:57,338 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-22500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:05:58,083 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-22500/spiece.model\n",
            "{'loss': 4.173, 'learning_rate': 3.137501012227711e-05, 'epoch': 0.37}\n",
            " 37% 23000/61745 [2:11:24<2:35:04,  4.16it/s][INFO|trainer.py:1503] 2021-02-21 18:08:43,071 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-23000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:08:43,079 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:08:49,645 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:08:49,651 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:08:49,655 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-23000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:08:50,478 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-23000/spiece.model\n",
            "{'loss': 4.2167, 'learning_rate': 3.0970119037978784e-05, 'epoch': 0.38}\n",
            " 38% 23500/61745 [2:14:15<3:03:52,  3.47it/s][INFO|trainer.py:1503] 2021-02-21 18:11:33,657 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-23500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:11:33,667 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:11:40,329 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:11:40,360 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:11:40,369 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-23500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:11:41,170 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-23500/spiece.model\n",
            "{'loss': 4.1839, 'learning_rate': 3.056522795368046e-05, 'epoch': 0.39}\n",
            " 39% 24000/61745 [2:17:06<2:47:14,  3.76it/s][INFO|trainer.py:1503] 2021-02-21 18:14:25,326 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-24000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:14:25,334 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:14:32,143 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:14:32,151 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:14:32,156 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-24000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:14:32,934 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-24000/spiece.model\n",
            "{'loss': 4.1718, 'learning_rate': 3.016033686938214e-05, 'epoch': 0.4}\n",
            " 40% 24500/61745 [2:19:57<2:46:03,  3.74it/s][INFO|trainer.py:1503] 2021-02-21 18:17:15,886 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-24500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:17:15,905 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:17:22,485 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:17:22,491 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:17:22,496 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-24500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:17:23,293 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-24500/spiece.model\n",
            "{'loss': 4.1826, 'learning_rate': 2.9755445785083813e-05, 'epoch': 0.4}\n",
            " 40% 25000/61745 [2:22:50<2:57:33,  3.45it/s][INFO|trainer.py:1503] 2021-02-21 18:20:09,408 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-25000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:20:09,416 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:20:16,117 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:20:16,123 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:20:16,128 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-25000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:20:16,915 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-25000/spiece.model\n",
            "{'loss': 4.1661, 'learning_rate': 2.9350554700785486e-05, 'epoch': 0.41}\n",
            " 41% 25500/61745 [2:25:41<2:42:31,  3.72it/s][INFO|trainer.py:1503] 2021-02-21 18:22:59,682 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-25500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:22:59,692 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:23:06,406 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:23:06,414 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:23:06,448 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-25500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:23:07,294 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-25500/spiece.model\n",
            "{'loss': 4.1356, 'learning_rate': 2.894566361648717e-05, 'epoch': 0.42}\n",
            " 42% 26000/61745 [2:28:29<2:32:15,  3.91it/s][INFO|trainer.py:1503] 2021-02-21 18:25:48,431 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-26000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:25:48,442 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:25:55,344 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:25:55,352 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:25:55,359 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-26000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:25:56,134 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-26000/spiece.model\n",
            "{'loss': 4.1768, 'learning_rate': 2.8540772532188842e-05, 'epoch': 0.43}\n",
            " 43% 26500/61745 [2:31:20<2:50:15,  3.45it/s][INFO|trainer.py:1503] 2021-02-21 18:28:38,622 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-26500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:28:38,636 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:28:45,900 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:28:45,906 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:28:45,911 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-26500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:28:46,720 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-26500/spiece.model\n",
            "{'loss': 4.1505, 'learning_rate': 2.8135881447890515e-05, 'epoch': 0.44}\n",
            " 44% 27000/61745 [2:34:12<2:53:38,  3.33it/s][INFO|trainer.py:1503] 2021-02-21 18:31:30,809 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-27000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:31:30,817 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:31:37,680 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:31:38,053 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:31:38,070 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-27000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:31:38,862 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-27000/spiece.model\n",
            "{'loss': 4.1711, 'learning_rate': 2.7730990363592198e-05, 'epoch': 0.45}\n",
            " 45% 27500/61745 [2:37:02<2:47:15,  3.41it/s][INFO|trainer.py:1503] 2021-02-21 18:34:21,262 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-27500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:34:21,272 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:34:28,057 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:34:28,063 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:34:28,069 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-27500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:34:28,885 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-27500/spiece.model\n",
            "{'loss': 4.1352, 'learning_rate': 2.732609927929387e-05, 'epoch': 0.45}\n",
            " 45% 28000/61745 [2:39:52<2:17:32,  4.09it/s][INFO|trainer.py:1503] 2021-02-21 18:37:10,965 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-28000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:37:10,982 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:37:17,281 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:37:17,287 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:37:17,292 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-28000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:37:18,100 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-28000/spiece.model\n",
            "{'loss': 4.1258, 'learning_rate': 2.6921208194995544e-05, 'epoch': 0.46}\n",
            " 46% 28500/61745 [2:42:49<2:28:30,  3.73it/s][INFO|trainer.py:1503] 2021-02-21 18:40:07,951 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-28500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:40:07,962 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:40:14,171 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:40:14,182 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:40:14,187 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-28500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:40:14,959 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-28500/spiece.model\n",
            "{'loss': 4.1332, 'learning_rate': 2.6516317110697227e-05, 'epoch': 0.47}\n",
            " 47% 29000/61745 [2:45:39<3:02:21,  2.99it/s][INFO|trainer.py:1503] 2021-02-21 18:42:58,437 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-29000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:42:58,534 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:43:42,029 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:43:42,130 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:43:42,224 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-29000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:43:43,618 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-29000/spiece.model\n",
            "{'loss': 4.1237, 'learning_rate': 2.61114260263989e-05, 'epoch': 0.48}\n",
            " 48% 29500/61745 [2:49:29<2:25:49,  3.69it/s][INFO|trainer.py:1503] 2021-02-21 18:46:47,693 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-29500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:46:47,706 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:46:54,088 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:46:54,094 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:46:54,100 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-29500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:46:54,910 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-29500/spiece.model\n",
            "{'loss': 4.1216, 'learning_rate': 2.5706534942100573e-05, 'epoch': 0.49}\n",
            " 49% 30000/61745 [2:52:24<2:23:59,  3.67it/s][INFO|trainer.py:1503] 2021-02-21 18:49:43,051 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-30000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:49:43,059 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:49:49,546 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:49:50,022 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:49:50,027 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-30000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:49:50,870 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-30000/spiece.model\n",
            "{'loss': 4.129, 'learning_rate': 2.5301643857802256e-05, 'epoch': 0.49}\n",
            " 49% 30500/61745 [2:55:18<2:24:32,  3.60it/s][INFO|trainer.py:1503] 2021-02-21 18:52:36,587 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-30500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:52:36,595 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:52:43,127 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:52:43,170 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:52:43,188 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-30500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:52:43,957 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-30500/spiece.model\n",
            "{'loss': 4.1446, 'learning_rate': 2.489675277350393e-05, 'epoch': 0.5}\n",
            " 50% 31000/61745 [2:58:10<2:29:32,  3.43it/s][INFO|trainer.py:1503] 2021-02-21 18:55:28,488 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-31000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:55:28,498 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:55:34,921 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:55:34,926 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:55:34,941 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-31000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:55:35,765 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-31000/spiece.model\n",
            "{'loss': 4.1052, 'learning_rate': 2.4491861689205605e-05, 'epoch': 0.51}\n",
            " 51% 31500/61745 [3:01:03<2:19:18,  3.62it/s][INFO|trainer.py:1503] 2021-02-21 18:58:21,672 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-31500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 18:58:21,779 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 18:58:38,389 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 18:58:38,404 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 18:58:38,410 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-31500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 18:58:39,230 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-31500/spiece.model\n",
            "{'loss': 4.1208, 'learning_rate': 2.408697060490728e-05, 'epoch': 0.52}\n",
            " 52% 32000/61745 [3:04:05<2:39:45,  3.10it/s][INFO|trainer.py:1503] 2021-02-21 19:01:23,979 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-32000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:01:24,065 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:01:44,533 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:01:44,609 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:01:44,618 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-32000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:01:45,807 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-32000/spiece.model\n",
            "{'loss': 4.1369, 'learning_rate': 2.3682079520608957e-05, 'epoch': 0.53}\n",
            " 53% 32500/61745 [3:07:04<2:27:30,  3.30it/s][INFO|trainer.py:1503] 2021-02-21 19:04:23,048 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-32500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:04:23,159 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:04:42,234 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:04:42,242 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:04:42,249 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-32500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:04:43,070 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-32500/spiece.model\n",
            "{'loss': 4.1137, 'learning_rate': 2.3277188436310634e-05, 'epoch': 0.53}\n",
            " 53% 33000/61745 [3:09:59<2:11:07,  3.65it/s][INFO|trainer.py:1503] 2021-02-21 19:07:18,007 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-33000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:07:18,097 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:07:39,844 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:07:39,878 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:07:39,952 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-33000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:07:40,958 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-33000/spiece.model\n",
            "{'loss': 4.111, 'learning_rate': 2.287229735201231e-05, 'epoch': 0.54}\n",
            " 54% 33500/61745 [3:12:57<2:18:20,  3.40it/s][INFO|trainer.py:1503] 2021-02-21 19:10:15,819 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-33500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:10:15,835 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:10:22,662 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:10:22,679 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:10:22,687 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-33500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:10:23,542 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-33500/spiece.model\n",
            "{'loss': 4.1067, 'learning_rate': 2.2467406267713986e-05, 'epoch': 0.55}\n",
            " 55% 34000/61745 [3:15:49<2:08:27,  3.60it/s][INFO|trainer.py:1503] 2021-02-21 19:13:07,827 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-34000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:13:08,032 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:13:54,370 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:13:54,474 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:13:54,581 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-34000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:13:55,979 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-34000/spiece.model\n",
            "{'loss': 4.1133, 'learning_rate': 2.2062515183415663e-05, 'epoch': 0.56}\n",
            " 56% 34500/61745 [3:19:11<1:52:42,  4.03it/s][INFO|trainer.py:1503] 2021-02-21 19:16:30,338 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-34500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:16:30,347 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:16:36,631 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:16:36,636 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:16:36,640 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-34500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:16:37,453 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-34500/spiece.model\n",
            "{'loss': 4.1196, 'learning_rate': 2.165762409911734e-05, 'epoch': 0.57}\n",
            " 57% 35000/61745 [3:22:03<2:25:46,  3.06it/s][INFO|trainer.py:1503] 2021-02-21 19:19:21,677 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-35000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:19:21,967 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:20:08,081 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:20:08,193 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:20:08,586 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-35000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:20:09,995 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-35000/spiece.model\n",
            "{'loss': 4.0789, 'learning_rate': 2.1252733014819015e-05, 'epoch': 0.57}\n",
            " 57% 35500/61745 [3:25:46<1:59:20,  3.67it/s][INFO|trainer.py:1503] 2021-02-21 19:23:05,381 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-35500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:23:05,492 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:23:16,128 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:23:16,134 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:23:16,141 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-35500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:23:16,955 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-35500/spiece.model\n",
            "{'loss': 4.0893, 'learning_rate': 2.0847841930520688e-05, 'epoch': 0.58}\n",
            " 58% 36000/61745 [3:28:35<1:59:57,  3.58it/s][INFO|trainer.py:1503] 2021-02-21 19:25:54,213 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-36000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:25:54,222 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:26:11,072 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:26:11,086 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:26:11,178 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-36000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:26:12,284 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-36000/spiece.model\n",
            "{'loss': 4.0559, 'learning_rate': 2.0442950846222368e-05, 'epoch': 0.59}\n",
            " 59% 36500/61745 [3:31:32<1:58:31,  3.55it/s][INFO|trainer.py:1503] 2021-02-21 19:28:51,288 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-36500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:28:51,491 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:29:51,675 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:29:51,797 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:29:51,978 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-36500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:29:53,380 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-36500/spiece.model\n",
            "{'loss': 4.0937, 'learning_rate': 2.0038059761924044e-05, 'epoch': 0.6}\n",
            " 60% 37000/61745 [3:35:24<1:53:22,  3.64it/s][INFO|trainer.py:1503] 2021-02-21 19:32:42,743 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-37000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:32:42,752 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:32:49,289 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:32:49,294 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:32:49,301 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-37000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:32:50,118 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-37000/spiece.model\n",
            "{'loss': 4.09, 'learning_rate': 1.9633168677625717e-05, 'epoch': 0.61}\n",
            " 61% 37500/61745 [3:38:16<1:39:31,  4.06it/s][INFO|trainer.py:1503] 2021-02-21 19:35:35,459 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-37500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:35:35,469 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:35:42,371 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:35:42,378 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:35:42,382 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-37500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:35:43,213 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-37500/spiece.model\n",
            "{'loss': 4.0717, 'learning_rate': 1.9228277593327397e-05, 'epoch': 0.62}\n",
            " 62% 38000/61745 [3:41:07<1:42:47,  3.85it/s][INFO|trainer.py:1503] 2021-02-21 19:38:26,178 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-38000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:38:26,188 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:38:32,655 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:38:32,661 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:38:32,677 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-38000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:38:33,426 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-38000/spiece.model\n",
            "{'loss': 4.1042, 'learning_rate': 1.8823386509029073e-05, 'epoch': 0.62}\n",
            " 62% 38500/61745 [3:44:07<1:49:58,  3.52it/s][INFO|trainer.py:1503] 2021-02-21 19:41:25,620 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-38500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:41:25,806 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:41:50,113 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:41:50,128 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:41:50,209 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-38500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:41:51,119 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-38500/spiece.model\n",
            "{'loss': 4.0732, 'learning_rate': 1.8418495424730746e-05, 'epoch': 0.63}\n",
            " 63% 39000/61745 [3:47:28<1:53:20,  3.34it/s][INFO|trainer.py:1503] 2021-02-21 19:44:46,667 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-39000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:44:46,679 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:44:53,423 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:44:53,459 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:44:53,473 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-39000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:44:54,243 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-39000/spiece.model\n",
            "{'loss': 4.0805, 'learning_rate': 1.8013604340432425e-05, 'epoch': 0.64}\n",
            " 64% 39500/61745 [3:50:19<1:40:18,  3.70it/s][INFO|trainer.py:1503] 2021-02-21 19:47:37,979 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-39500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:47:38,186 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:48:21,163 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:48:21,277 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:48:21,370 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-39500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:48:22,587 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-39500/spiece.model\n",
            "{'loss': 4.0752, 'learning_rate': 1.7608713256134102e-05, 'epoch': 0.65}\n",
            " 65% 40000/61745 [3:53:54<1:38:13,  3.69it/s][INFO|trainer.py:1503] 2021-02-21 19:51:13,450 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-40000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:51:13,466 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:51:20,294 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:51:20,307 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:51:20,314 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-40000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:51:21,166 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-40000/spiece.model\n",
            "{'loss': 4.0971, 'learning_rate': 1.7203822171835775e-05, 'epoch': 0.66}\n",
            " 66% 40500/61745 [3:56:53<1:41:52,  3.48it/s][INFO|trainer.py:1503] 2021-02-21 19:54:11,777 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-40500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:54:11,795 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:54:19,007 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:54:19,015 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:54:19,019 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-40500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:54:19,806 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-40500/spiece.model\n",
            "{'loss': 4.0787, 'learning_rate': 1.6798931087537454e-05, 'epoch': 0.66}\n",
            " 66% 41000/61745 [3:59:43<1:51:34,  3.10it/s][INFO|trainer.py:1503] 2021-02-21 19:57:02,340 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-41000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 19:57:02,449 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 19:57:49,246 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 19:57:49,442 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 19:57:49,549 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-41000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 19:57:51,048 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-41000/spiece.model\n",
            "{'loss': 4.0545, 'learning_rate': 1.639404000323913e-05, 'epoch': 0.67}\n",
            " 67% 41500/61745 [4:03:16<1:37:13,  3.47it/s][INFO|trainer.py:1503] 2021-02-21 20:00:35,467 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-41500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:00:35,477 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:00:42,128 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:00:42,134 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:00:42,139 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-41500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:00:42,959 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-41500/spiece.model\n",
            "{'loss': 4.0501, 'learning_rate': 1.5989148918940803e-05, 'epoch': 0.68}\n",
            " 68% 42000/61745 [4:06:06<1:21:17,  4.05it/s][INFO|trainer.py:1503] 2021-02-21 20:03:25,462 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-42000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:03:25,489 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:03:31,964 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:03:31,969 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:03:31,974 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-42000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:03:32,774 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-42000/spiece.model\n",
            "{'loss': 4.07, 'learning_rate': 1.5584257834642483e-05, 'epoch': 0.69}\n",
            " 69% 42500/61745 [4:09:03<1:36:55,  3.31it/s][INFO|trainer.py:1503] 2021-02-21 20:06:22,612 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-42500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:06:22,812 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:06:51,689 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:06:51,726 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:06:51,732 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-42500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:06:52,515 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-42500/spiece.model\n",
            "{'loss': 4.0516, 'learning_rate': 1.517936675034416e-05, 'epoch': 0.7}\n",
            " 70% 43000/61745 [4:12:09<1:30:37,  3.45it/s][INFO|trainer.py:1503] 2021-02-21 20:09:27,834 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-43000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:09:27,935 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:09:46,163 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:09:46,170 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:09:46,176 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-43000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:09:46,914 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-43000/spiece.model\n",
            "{'loss': 4.0584, 'learning_rate': 1.4774475666045834e-05, 'epoch': 0.7}\n",
            " 70% 43500/61745 [4:15:00<1:13:52,  4.12it/s][INFO|trainer.py:1503] 2021-02-21 20:12:18,766 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-43500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:12:18,775 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:12:25,420 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:12:25,425 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:12:25,433 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-43500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:12:26,177 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-43500/spiece.model\n",
            "{'loss': 4.063, 'learning_rate': 1.436958458174751e-05, 'epoch': 0.71}\n",
            " 71% 44000/61745 [4:17:51<1:19:59,  3.70it/s][INFO|trainer.py:1503] 2021-02-21 20:15:10,433 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-44000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:15:10,606 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:16:33,948 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:16:34,143 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:16:34,346 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-44000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:16:36,245 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-44000/spiece.model\n",
            "{'loss': 4.0606, 'learning_rate': 1.3964693497449185e-05, 'epoch': 0.72}\n",
            " 72% 44500/61745 [4:22:58<1:15:14,  3.82it/s][INFO|trainer.py:1503] 2021-02-21 20:20:16,704 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-44500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:20:16,715 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:20:22,741 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:20:22,748 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:20:22,752 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-44500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:20:23,573 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-44500/spiece.model\n",
            "{'loss': 4.0502, 'learning_rate': 1.3559802413150863e-05, 'epoch': 0.73}\n",
            " 73% 45000/61745 [4:25:56<1:18:58,  3.53it/s][INFO|trainer.py:1503] 2021-02-21 20:23:15,133 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-45000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:23:15,145 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:23:21,535 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:23:21,542 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:23:21,551 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-45000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:23:22,365 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-45000/spiece.model\n",
            "{'loss': 4.0591, 'learning_rate': 1.3154911328852539e-05, 'epoch': 0.74}\n",
            " 74% 45500/61745 [4:28:51<1:09:50,  3.88it/s][INFO|trainer.py:1503] 2021-02-21 20:26:09,551 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-45500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:26:09,577 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:26:27,007 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:26:27,078 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:26:27,088 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-45500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:26:28,185 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-45500/spiece.model\n",
            "{'loss': 4.0645, 'learning_rate': 1.2750020244554214e-05, 'epoch': 0.74}\n",
            " 74% 46000/61745 [4:31:45<1:18:46,  3.33it/s][INFO|trainer.py:1503] 2021-02-21 20:29:03,489 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-46000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:29:03,498 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:29:09,998 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:29:10,008 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:29:10,014 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-46000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:29:10,870 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-46000/spiece.model\n",
            "{'loss': 4.069, 'learning_rate': 1.2345129160255892e-05, 'epoch': 0.75}\n",
            " 75% 46500/61745 [4:34:40<1:19:04,  3.21it/s][INFO|trainer.py:1503] 2021-02-21 20:31:58,929 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-46500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:31:59,046 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:32:48,997 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:32:49,006 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:32:49,013 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-46500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:32:49,744 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-46500/spiece.model\n",
            "{'loss': 4.0735, 'learning_rate': 1.1940238075957568e-05, 'epoch': 0.76}\n",
            " 76% 47000/61745 [4:38:06<1:10:17,  3.50it/s][INFO|trainer.py:1503] 2021-02-21 20:35:25,033 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-47000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:35:25,132 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:35:44,959 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:35:45,043 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:35:45,059 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-47000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:35:46,042 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-47000/spiece.model\n",
            "{'loss': 4.0413, 'learning_rate': 1.1535346991659244e-05, 'epoch': 0.77}\n",
            " 77% 47500/61745 [4:41:06<1:13:34,  3.23it/s][INFO|trainer.py:1503] 2021-02-21 20:38:25,134 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-47500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:38:25,162 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-47500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:38:31,505 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-47500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:38:31,511 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-47500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:38:31,515 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-47500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:38:32,249 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-47500/spiece.model\n",
            "{'loss': 4.0583, 'learning_rate': 1.113045590736092e-05, 'epoch': 0.78}\n",
            " 78% 48000/61745 [4:44:00<59:41,  3.84it/s][INFO|trainer.py:1503] 2021-02-21 20:41:18,831 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-48000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:41:18,839 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-48000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:41:24,889 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-48000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:41:24,895 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-48000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:41:24,899 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-48000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:41:25,700 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-48000/spiece.model\n",
            "{'loss': 4.0343, 'learning_rate': 1.0725564823062597e-05, 'epoch': 0.79}\n",
            " 79% 48500/61745 [4:46:54<1:08:50,  3.21it/s][INFO|trainer.py:1503] 2021-02-21 20:44:12,796 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-48500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:44:12,899 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-48500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:44:53,698 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-48500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:44:53,794 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-48500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:44:54,094 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-48500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:44:55,294 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-48500/spiece.model\n",
            "{'loss': 4.0545, 'learning_rate': 1.0320673738764273e-05, 'epoch': 0.79}\n",
            " 79% 49000/61745 [4:50:37<55:42,  3.81it/s][INFO|trainer.py:1503] 2021-02-21 20:47:55,961 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-49000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:47:55,972 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-49000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:48:02,832 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-49000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:48:02,838 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-49000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:48:02,844 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-49000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:48:03,546 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-49000/spiece.model\n",
            "{'loss': 4.0312, 'learning_rate': 9.91578265446595e-06, 'epoch': 0.8}\n",
            " 80% 49500/61745 [4:53:32<57:30,  3.55it/s][INFO|trainer.py:1503] 2021-02-21 20:50:51,339 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-49500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:50:51,348 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-49500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:50:58,068 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-49500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:50:58,074 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-49500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:50:58,081 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-49500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:50:58,809 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-49500/spiece.model\n",
            "{'loss': 4.0495, 'learning_rate': 9.510891570167626e-06, 'epoch': 0.81}\n",
            " 81% 50000/61745 [4:56:33<52:36,  3.72it/s][INFO|trainer.py:1503] 2021-02-21 20:53:52,010 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-50000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:53:52,021 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:54:01,183 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:54:01,238 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:54:01,248 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-50000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:54:02,348 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-50000/spiece.model\n",
            "{'loss': 4.0427, 'learning_rate': 9.1060004858693e-06, 'epoch': 0.82}\n",
            " 82% 50500/61745 [4:59:38<51:47,  3.62it/s][INFO|trainer.py:1503] 2021-02-21 20:56:56,879 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-50500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:56:56,891 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-50500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:57:03,548 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-50500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:57:03,556 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-50500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:57:03,883 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-50500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:57:04,677 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-50500/spiece.model\n",
            "{'loss': 4.0497, 'learning_rate': 8.701109401570978e-06, 'epoch': 0.83}\n",
            " 83% 51000/61745 [5:02:30<47:24,  3.78it/s][INFO|trainer.py:1503] 2021-02-21 20:59:49,379 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-51000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 20:59:49,405 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-51000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 20:59:55,570 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-51000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 20:59:55,577 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-51000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 20:59:55,589 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-51000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 20:59:56,397 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-51000/spiece.model\n",
            "{'loss': 4.0407, 'learning_rate': 8.296218317272655e-06, 'epoch': 0.83}\n",
            " 83% 51500/61745 [5:05:23<47:19,  3.61it/s][INFO|trainer.py:1503] 2021-02-21 21:02:41,865 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-51500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:02:41,874 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-51500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:02:48,233 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-51500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:02:48,239 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-51500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:02:48,244 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-51500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:02:48,971 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-51500/spiece.model\n",
            "{'loss': 4.0613, 'learning_rate': 7.891327232974329e-06, 'epoch': 0.84}\n",
            " 84% 52000/61745 [5:08:15<39:49,  4.08it/s][INFO|trainer.py:1503] 2021-02-21 21:05:33,742 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-52000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:05:33,753 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-52000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:05:39,895 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-52000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:05:39,903 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-52000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:05:40,343 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-52000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:05:41,051 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-52000/spiece.model\n",
            "{'loss': 4.0378, 'learning_rate': 7.486436148676007e-06, 'epoch': 0.85}\n",
            " 85% 52500/61745 [5:11:09<40:37,  3.79it/s][INFO|trainer.py:1503] 2021-02-21 21:08:27,655 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-52500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:08:27,663 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-52500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:08:34,034 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-52500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:08:34,040 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-52500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:08:34,047 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-52500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:08:34,771 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-52500/spiece.model\n",
            "{'loss': 4.0187, 'learning_rate': 7.0815450643776825e-06, 'epoch': 0.86}\n",
            " 86% 53000/61745 [5:14:00<42:18,  3.45it/s][INFO|trainer.py:1503] 2021-02-21 21:11:18,972 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-53000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:11:18,981 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-53000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:11:24,914 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-53000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:11:24,933 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-53000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:11:24,939 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-53000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:11:25,687 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-53000/spiece.model\n",
            "{'loss': 4.0378, 'learning_rate': 6.676653980079359e-06, 'epoch': 0.87}\n",
            " 87% 53500/61745 [5:16:57<42:00,  3.27it/s][INFO|trainer.py:1503] 2021-02-21 21:14:16,375 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-53500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:14:16,678 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-53500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:16:12,127 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-53500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:16:12,426 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-53500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:16:12,630 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-53500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:16:14,524 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-53500/spiece.model\n",
            "{'loss': 4.0326, 'learning_rate': 6.271762895781034e-06, 'epoch': 0.87}\n",
            " 87% 54000/61745 [5:23:06<35:42,  3.61it/s][INFO|trainer.py:1503] 2021-02-21 21:20:25,101 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-54000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:20:25,107 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-54000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:20:39,794 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-54000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:20:39,817 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-54000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:20:39,906 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-54000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:20:41,003 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-54000/spiece.model\n",
            "{'loss': 4.0325, 'learning_rate': 5.866871811482711e-06, 'epoch': 0.88}\n",
            " 88% 54500/61745 [5:26:04<32:56,  3.67it/s][INFO|trainer.py:1503] 2021-02-21 21:23:22,770 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-54500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:23:22,866 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-54500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:23:43,474 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-54500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:23:43,548 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-54500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:23:43,558 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-54500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:23:44,650 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-54500/spiece.model\n",
            "{'loss': 4.0394, 'learning_rate': 5.461980727184388e-06, 'epoch': 0.89}\n",
            " 89% 55000/61745 [5:28:59<27:42,  4.06it/s][INFO|trainer.py:1503] 2021-02-21 21:26:17,643 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-55000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:26:17,739 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-55000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:26:31,583 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-55000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:26:31,590 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-55000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:26:31,594 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-55000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:26:32,423 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-55000/spiece.model\n",
            "{'loss': 4.0499, 'learning_rate': 5.057089642886064e-06, 'epoch': 0.9}\n",
            " 90% 55500/61745 [5:31:50<28:06,  3.70it/s][INFO|trainer.py:1503] 2021-02-21 21:29:08,984 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-55500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:29:09,079 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-55500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:29:21,934 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-55500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:29:21,991 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-55500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:29:21,999 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-55500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:29:22,746 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-55500/spiece.model\n",
            "{'loss': 4.0486, 'learning_rate': 4.65219855858774e-06, 'epoch': 0.91}\n",
            " 91% 56000/61745 [5:34:51<26:25,  3.62it/s][INFO|trainer.py:1503] 2021-02-21 21:32:09,566 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-56000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:32:09,658 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-56000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:32:30,375 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-56000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:32:30,445 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-56000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:32:30,452 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-56000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:32:31,401 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-56000/spiece.model\n",
            "{'loss': 4.0513, 'learning_rate': 4.2473074742894165e-06, 'epoch': 0.92}\n",
            " 92% 56500/61745 [5:37:55<26:17,  3.33it/s][INFO|trainer.py:1503] 2021-02-21 21:35:14,239 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-56500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:35:14,253 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-56500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:35:21,408 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-56500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:35:21,446 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-56500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:35:22,015 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-56500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:35:22,765 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-56500/spiece.model\n",
            "{'loss': 4.032, 'learning_rate': 3.842416389991092e-06, 'epoch': 0.92}\n",
            " 92% 57000/61745 [5:40:48<22:53,  3.45it/s][INFO|trainer.py:1503] 2021-02-21 21:38:06,649 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-57000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:38:06,660 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-57000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:38:13,060 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-57000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:38:13,066 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-57000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:38:13,092 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-57000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:38:13,882 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-57000/spiece.model\n",
            "{'loss': 4.0452, 'learning_rate': 3.4375253056927687e-06, 'epoch': 0.93}\n",
            " 93% 57500/61745 [5:44:05<22:15,  3.18it/s][INFO|trainer.py:1503] 2021-02-21 21:41:23,816 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-57500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:41:23,907 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-57500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:41:41,031 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-57500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:41:41,038 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-57500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:41:41,044 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-57500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:41:42,027 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-57500/spiece.model\n",
            "{'loss': 4.0359, 'learning_rate': 3.032634221394445e-06, 'epoch': 0.94}\n",
            " 94% 58000/61745 [5:47:13<20:43,  3.01it/s][INFO|trainer.py:1503] 2021-02-21 21:44:32,331 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-58000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:44:32,343 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-58000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:44:38,933 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-58000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:44:39,541 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-58000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:44:39,545 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-58000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:44:40,242 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-58000/spiece.model\n",
            "{'loss': 4.0438, 'learning_rate': 2.6277431370961212e-06, 'epoch': 0.95}\n",
            " 95% 58500/61745 [5:50:07<14:29,  3.73it/s][INFO|trainer.py:1503] 2021-02-21 21:47:25,721 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-58500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:47:25,733 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-58500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:47:32,324 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-58500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:47:32,340 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-58500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:47:32,345 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-58500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:47:33,144 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-58500/spiece.model\n",
            "{'loss': 4.0363, 'learning_rate': 2.2228520527977975e-06, 'epoch': 0.96}\n",
            " 96% 59000/61745 [5:52:58<10:58,  4.17it/s][INFO|trainer.py:1503] 2021-02-21 21:50:17,001 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-59000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:50:17,009 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-59000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:50:23,190 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-59000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:50:23,197 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-59000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:50:23,202 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-59000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:50:24,021 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-59000/spiece.model\n",
            "{'loss': 4.0562, 'learning_rate': 1.8179609684994738e-06, 'epoch': 0.96}\n",
            " 96% 59500/61745 [5:55:51<10:44,  3.48it/s][INFO|trainer.py:1503] 2021-02-21 21:53:10,462 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-59500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:53:10,656 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-59500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:53:49,153 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-59500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:53:49,246 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-59500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:53:49,346 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-59500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:53:50,755 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-59500/spiece.model\n",
            "{'loss': 4.036, 'learning_rate': 1.41306988420115e-06, 'epoch': 0.97}\n",
            " 97% 60000/61745 [5:59:26<07:35,  3.83it/s][INFO|trainer.py:1503] 2021-02-21 21:56:44,874 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-60000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:56:44,900 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-60000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 21:57:01,402 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 21:57:01,417 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 21:57:01,503 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-60000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 21:57:02,699 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-60000/spiece.model\n",
            "{'loss': 4.0359, 'learning_rate': 1.0081787999028262e-06, 'epoch': 0.98}\n",
            " 98% 60500/61745 [6:02:38<05:46,  3.60it/s][INFO|trainer.py:1503] 2021-02-21 21:59:57,227 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-60500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 21:59:57,322 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-60500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 22:00:18,863 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-60500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 22:00:18,874 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-60500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 22:00:18,980 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-60500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 22:00:20,071 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-60500/spiece.model\n",
            "{'loss': 4.0497, 'learning_rate': 6.032877156045024e-07, 'epoch': 0.99}\n",
            " 99% 61000/61745 [6:05:51<03:36,  3.45it/s][INFO|trainer.py:1503] 2021-02-21 22:03:09,869 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-61000\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 22:03:09,881 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-61000/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 22:03:28,379 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-61000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 22:03:28,400 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-61000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 22:03:28,493 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-61000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 22:03:29,501 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-61000/spiece.model\n",
            "{'loss': 4.0256, 'learning_rate': 1.9839663130617865e-07, 'epoch': 1.0}\n",
            "100% 61500/61745 [6:08:52<01:00,  4.07it/s][INFO|trainer.py:1503] 2021-02-21 22:06:10,513 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model/checkpoint-61500\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 22:06:10,522 >> Configuration saved in /content/drive/MyDrive/rsse_model/checkpoint-61500/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 22:06:17,048 >> Model weights saved in /content/drive/MyDrive/rsse_model/checkpoint-61500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 22:06:17,058 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/checkpoint-61500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 22:06:17,066 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/checkpoint-61500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 22:06:17,918 >> Copy vocab file to /content/drive/MyDrive/rsse_model/checkpoint-61500/spiece.model\n",
            "100% 61745/61745 [6:10:36<00:00,  3.16it/s][INFO|trainer.py:1063] 2021-02-21 22:07:54,794 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 22236.349, 'train_samples_per_second': 2.777, 'epoch': 1.0}\n",
            "100% 61745/61745 [6:10:36<00:00,  2.78it/s]\n",
            "[INFO|trainer.py:1503] 2021-02-21 22:07:56,243 >> Saving model checkpoint to /content/drive/MyDrive/rsse_model\n",
            "[INFO|configuration_utils.py:311] 2021-02-21 22:07:56,331 >> Configuration saved in /content/drive/MyDrive/rsse_model/config.json\n",
            "[INFO|modeling_utils.py:825] 2021-02-21 22:08:05,177 >> Model weights saved in /content/drive/MyDrive/rsse_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 22:08:05,184 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 22:08:05,189 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 22:08:05,879 >> Copy vocab file to /content/drive/MyDrive/rsse_model/spiece.model\n",
            "02/21/2021 22:08:05 - INFO - __main__ -   ***** train metrics *****\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     epoch = 1.0\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     init_mem_cpu_alloc_delta = 322870\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     init_mem_cpu_peaked_delta = 18282\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     init_mem_gpu_alloc_delta = 1200707584\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     init_mem_gpu_peaked_delta = 0\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_mem_cpu_alloc_delta = 265858728\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_mem_cpu_peaked_delta = 1043361652\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_mem_gpu_alloc_delta = 1843786240\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_mem_gpu_peaked_delta = 4786723840\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_n_objs = -1\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_runtime = 22236.349\n",
            "02/21/2021 22:08:05 - INFO - __main__ -     train_samples_per_second = 2.777\n",
            "[INFO|tokenization_utils_base.py:1980] 2021-02-21 22:08:05,904 >> tokenizer config file saved in /content/drive/MyDrive/rsse_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1986] 2021-02-21 22:08:05,908 >> Special tokens file saved in /content/drive/MyDrive/rsse_model/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:163] 2021-02-21 22:08:06,257 >> Copy vocab file to /content/drive/MyDrive/rsse_model/spiece.model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}